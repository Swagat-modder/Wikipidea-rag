{"docstore/metadata": {"233488": {"doc_hash": "12e75e56610f6a3e55b7def2899d987437eeca791453a796249ee4a51d739b23"}, "40409788": {"doc_hash": "e3dc56b9ef42af1f1a69930bccf0dad09e4ff8d7b0e8e4a7e4ef9ff14c03a388"}, "e19629e2-aa3b-4eac-be61-af4e39110824": {"doc_hash": "2b008e9fd7aeef7190339ef78521b2e363db1abb4f4333359e31af124632ab9a", "ref_doc_id": "233488"}, "868a8cb0-3c5e-4def-b00b-56e90ca0de44": {"doc_hash": "6c1b73a39dcc5f14a810e8aa7e0e0af1dfada7c39a93a80fef62b14027f0852d", "ref_doc_id": "233488"}, "d494ccf7-7264-4781-8bb0-2d4420ec73d8": {"doc_hash": "83511b761af202f82e7d8b52354150f8b242203513d58530f48a3d3e93ffce48", "ref_doc_id": "233488"}, "1912ec31-be5e-454f-96f1-3122610cfb18": {"doc_hash": "9a0c7171cc47b73fc76f6c7aa073c8f2c20f61a6cf8ed8174493b8d9ea87d6e4", "ref_doc_id": "233488"}, "eb30143a-e361-4f5b-9188-a63d2f9bffb2": {"doc_hash": "66e5d151c42e890f2eeb2fd6c47e9f07d2fbaa45326fe68730c1b3a7e2061b54", "ref_doc_id": "233488"}, "fd6869ce-427d-409f-ae3d-ba81d9e7bc4b": {"doc_hash": "b34717786220a0887c24cfaa2d6710db96795ade1833c7c4fc3d78842231cbbc", "ref_doc_id": "233488"}, "854b52f3-a071-4de2-a390-59ee4dce7694": {"doc_hash": "38493ce3c337dbf140869bf6a40770a001021913960477a18fe3208928e53671", "ref_doc_id": "233488"}, "baf65ae8-78ab-4f79-bbee-240ad42fd2fd": {"doc_hash": "5f064d7f0cdbf86d5135a1514be46a74712a14cb06a57a6fb4f0df6dc7eb6980", "ref_doc_id": "233488"}, "b9811952-a229-4c7b-b04e-baa78fd8419e": {"doc_hash": "068e7b69ef508fa775bda8e370f6894567fe6bce251a3dbbd65d39211132156f", "ref_doc_id": "233488"}, "6cd378c8-683e-449f-8b72-e4df40ce35d5": {"doc_hash": "d261f12302f075c79305f7a688d9acb6b9a6ef6550adf6f7d09ff99c5d53529a", "ref_doc_id": "233488"}, "cf1e9d36-e7eb-4d6f-b578-b423df61c8af": {"doc_hash": "bbefd6f6935096165fbbebac2921c94ecaa22e479153937367bc5c2606932f09", "ref_doc_id": "233488"}, "398628d2-320e-4a1b-9bac-9e94455478e6": {"doc_hash": "bef16661358e0a4c0e51d3f2bc1eac8ab23bdd915c0400d141aec5b712fe087f", "ref_doc_id": "233488"}, "5c27ec39-cd53-4b67-bd76-92d14f4d9412": {"doc_hash": "fde3e47dc78cad06ddfa255e8aedca745233072652929a34d38078c1a43b9c7d", "ref_doc_id": "233488"}, "5b76db97-4452-45bc-8331-0272147563ac": {"doc_hash": "bff1ffc7829a4df7a9f0a99b2f40bab260abe2c38a7698cb8ce9c77184b12417", "ref_doc_id": "40409788"}, "c17c2500-6152-48ba-96e5-5e34a2575c66": {"doc_hash": "ec25a31db27e14def7d2876f2be3b484fc90731068663d84ebc1819cc9e8425f", "ref_doc_id": "40409788"}, "a6f522fa-dd3d-474a-83f9-cdc03e15cf4d": {"doc_hash": "bda2ebb583c9a8b676b2ed17a0659b8ce0f799f84dc1716f9625a1c6cfa61bf1", "ref_doc_id": "40409788"}, "2047fa83-039b-476e-8088-0481597eb1ba": {"doc_hash": "2f251fa8d7d1e0c52028303717f9715af1e44bb4769fe1d5722d9947c7d65a16", "ref_doc_id": "40409788"}, "e231a92a-1955-453c-8dcc-6e1d242d43f0": {"doc_hash": "007afd1be5de0db82730cee4e08eac3223697ed7d8cc657a2c6d95ec54e373d9", "ref_doc_id": "40409788"}, "c20c5fdf-2b00-4190-8bfa-f9b2502d56ff": {"doc_hash": "d7f3665ce0568050df9042dc14d2b20f266f9ef4d639df8277fe5fe3db1dbed7", "ref_doc_id": "40409788"}, "704d88ea-295a-4133-93a6-57e65f92611b": {"doc_hash": "d669ab12718c1502c2854ed11cc11c14f95f65bf81edfce7bd40333927f7f4f4", "ref_doc_id": "40409788"}, "77a85f82-e1f8-411c-8b92-662ebbb17272": {"doc_hash": "702551c10d7f7170d374907a743e0776b4cac616b46e25a4ecdbd88ed8f4d3b0", "ref_doc_id": "40409788"}, "a574c213-5872-49ca-91f6-fcc13a2c845f": {"doc_hash": "caa1afbf2cacc6b06ab77bf006b420a4edd36df99d7ef35f3a3512cb05ba4052", "ref_doc_id": "40409788"}, "02797ee9-f18b-43d9-8c32-466a0b820503": {"doc_hash": "d90c2cbdf4d09b492ee2ea267befbd3b36de85f40b3864c323fbca0c255ecfaf", "ref_doc_id": "40409788"}, "d6cc507a-8204-49e1-92dc-e332369ae740": {"doc_hash": "252de977c7e4754b3005c9a4608b805345e90dc118908f8e879ee2798f2693e2", "ref_doc_id": "40409788"}, "c185d241-e0d3-425c-b0f5-576dcf2241fd": {"doc_hash": "aaad958f2a02f0aa6beb7bced4a98b5ef621274e5424304d2c7647d42313945c", "ref_doc_id": "40409788"}, "df77e359-bc55-4b8a-a505-60bd016b07c7": {"doc_hash": "f914196a2f4d0f4742ac2d2ddd83556efd8581746726cf79c286bd968cd8fb19", "ref_doc_id": "40409788"}, "9cdc96cf-4934-4d7b-9a92-39bcdb005977": {"doc_hash": "703a30f66fb0b7dd4496e96fa97e0b800bd3ea1454d47c3e5508816479c50854", "ref_doc_id": "40409788"}, "ec1046e5-830b-4f74-9d67-13d01d1ecfdf": {"doc_hash": "b163152590e9ffbcb338e2d964e03e99ff4c9fddb445dfaf4b7c100ea6f0af1f", "ref_doc_id": "40409788"}, "b32db49f-4202-46c2-97f5-537a61a58ec2": {"doc_hash": "0cfa6b15c6a0b00307622d53db5d7d8dc3caca30abfd65393dbc14d76215e434", "ref_doc_id": "40409788"}, "286b5859-fd04-48da-af5b-538650bfcbd0": {"doc_hash": "52c992e9cb923aaf5cee0aca8819eed9c8a036ffacd4aecc5ead7dd923f53bf3", "ref_doc_id": "40409788"}}, "docstore/data": {"e19629e2-aa3b-4eac-be61-af4e39110824": {"__data__": {"id_": "e19629e2-aa3b-4eac-be61-af4e39110824", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "233488", "node_type": "4", "metadata": {}, "hash": "12e75e56610f6a3e55b7def2899d987437eeca791453a796249ee4a51d739b23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "868a8cb0-3c5e-4def-b00b-56e90ca0de44", "node_type": "1", "metadata": {}, "hash": "6c1b73a39dcc5f14a810e8aa7e0e0af1dfada7c39a93a80fef62b14027f0852d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\nML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\nStatistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. \nFrom a theoretical viewpoint, probably approximately correct learning provides a framework for describing machine learning.\n\n\n== History ==\n\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.\nThe earliest machine learning program was introduced in the 1950s when Arthur Samuel invented a computer program that calculated the winning chance in checkers for each side, but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.\nBy the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.\nTom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".\nModern-day machine learning has two objectives.  One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions.\n\n\n== Relationships to other fields ==", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4398, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "868a8cb0-3c5e-4def-b00b-56e90ca0de44": {"__data__": {"id_": "868a8cb0-3c5e-4def-b00b-56e90ca0de44", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "233488", "node_type": "4", "metadata": {}, "hash": "12e75e56610f6a3e55b7def2899d987437eeca791453a796249ee4a51d739b23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e19629e2-aa3b-4eac-be61-af4e39110824", "node_type": "1", "metadata": {}, "hash": "2b008e9fd7aeef7190339ef78521b2e363db1abb4f4333359e31af124632ab9a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d494ccf7-7264-4781-8bb0-2d4420ec73d8", "node_type": "1", "metadata": {}, "hash": "83511b761af202f82e7d8b52354150f8b242203513d58530f48a3d3e93ffce48", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "== Relationships to other fields ==\n\n\n=== Artificial intelligence ===\n\nAs a scientific endeavour, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalised linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis.:\u200a488\u200a\nHowever, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.:\u200a488\u200a By 1980, expert systems had come to dominate AI, and statistics was out of favour. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.:\u200a708\u2013710,\u200a755\u200a Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.:\u200a25\u200a\nMachine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.\n\n\n=== Data compression ===\n\n\n=== Data mining ===\nMachine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\nMachine learning also has intimate ties to optimisation: Many learning problems are formulated as minimisation of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples).\n\n\n=== Generalization ===\nCharacterizing the generalisation of various learning algorithms is an active topic of current research, especially for deep learning algorithms.\n\n\n=== Statistics ===\nMachine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalisable predictive patterns. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics. He also suggested the term data science as a placeholder to call the overall field.\nConventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.\nLeo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model, wherein \"algorithmic model\" means more or less the machine learning algorithms like Random Forest.\nSome statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.", "mimetype": "text/plain", "start_char_idx": 4363, "end_char_idx": 9549, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d494ccf7-7264-4781-8bb0-2d4420ec73d8": {"__data__": {"id_": "d494ccf7-7264-4781-8bb0-2d4420ec73d8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "233488", "node_type": "4", "metadata": {}, "hash": "12e75e56610f6a3e55b7def2899d987437eeca791453a796249ee4a51d739b23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "868a8cb0-3c5e-4def-b00b-56e90ca0de44", "node_type": "1", "metadata": {}, "hash": "6c1b73a39dcc5f14a810e8aa7e0e0af1dfada7c39a93a80fef62b14027f0852d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1912ec31-be5e-454f-96f1-3122610cfb18", "node_type": "1", "metadata": {}, "hash": "9a0c7171cc47b73fc76f6c7aa073c8f2c20f61a6cf8ed8174493b8d9ea87d6e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "=== Statistical physics ===\nAnalytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyse the weight space of deep neural networks. Statistical physics is thus finding applications in the area of medical diagnostics.\n\n\n== Theory ==\n\nA core objective of a learner is to generalise from its experience. Generalisation in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\nThe computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the probably approximately correct learning  model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias\u2013variance decomposition is one way to quantify generalisation error.\nFor the best performance in the context of generalisation, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalisation will be poorer.\nIn addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\n\n\n== Approaches ==\n\nMachine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system:\n\nSupervised learning: The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that maps inputs to outputs.\nUnsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).\nReinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximise.\nAlthough each algorithm has advantages and limitations, no single algorithm works for all problems.\n\n\n=== Supervised learning ===\n\nSupervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimisation of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.\nTypes of supervised-learning algorithms include active learning, classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range. For example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email. In contrast, regression is used for tasks such as predicting a person's height based on factors like age and genetics or forecasting future temperatures based on historical data.\nSimilarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.", "mimetype": "text/plain", "start_char_idx": 9552, "end_char_idx": 14819, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1912ec31-be5e-454f-96f1-3122610cfb18": {"__data__": {"id_": "1912ec31-be5e-454f-96f1-3122610cfb18", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "233488", "node_type": "4", "metadata": {}, "hash": "12e75e56610f6a3e55b7def2899d987437eeca791453a796249ee4a51d739b23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d494ccf7-7264-4781-8bb0-2d4420ec73d8", "node_type": "1", "metadata": {}, "hash": "83511b761af202f82e7d8b52354150f8b242203513d58530f48a3d3e93ffce48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb30143a-e361-4f5b-9188-a63d2f9bffb2", "node_type": "1", "metadata": {}, "hash": "66e5d151c42e890f2eeb2fd6c47e9f07d2fbaa45326fe68730c1b3a7e2061b54", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "=== Unsupervised learning ===\n\nUnsupervised learning algorithms find structures in data that has not been labelled, classified or categorised. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction, and density estimation.\nCluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.\nA special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself.\n\n\n=== Semi-supervised learning ===\n\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy.\nIn weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.\n\n\n=== Reinforcement learning ===\n\nReinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.\n\n\n=== Dimensionality reduction ===\nDimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables. In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D).\nThe manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularisation.\n\n\n=== Other types ===\nOther approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example, topic modelling, meta-learning.\n\n\n==== Self-learning ====\nSelf-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.\nThe self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: \n\nin situation s perform action a\nreceive a consequence situation s'\ncompute emotion of being in the consequence situation v(s')\nupdate crossbar memory  w'(a,s) = w(a,s) + v(s')\nIt is a system with only one input, situation, and only one output, action (or behaviour) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioural environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioural environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behaviour, in an environment that contains both desirable and undesirable situations.", "mimetype": "text/plain", "start_char_idx": 14822, "end_char_idx": 20318, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eb30143a-e361-4f5b-9188-a63d2f9bffb2": {"__data__": {"id_": "eb30143a-e361-4f5b-9188-a63d2f9bffb2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "233488", "node_type": "4", "metadata": {}, "hash": "12e75e56610f6a3e55b7def2899d987437eeca791453a796249ee4a51d739b23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1912ec31-be5e-454f-96f1-3122610cfb18", "node_type": "1", "metadata": {}, "hash": "9a0c7171cc47b73fc76f6c7aa073c8f2c20f61a6cf8ed8174493b8d9ea87d6e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd6869ce-427d-409f-ae3d-ba81d9e7bc4b", "node_type": "1", "metadata": {}, "hash": "b34717786220a0887c24cfaa2d6710db96795ade1833c7c4fc3d78842231cbbc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "==== Feature learning ====\n\nSeveral learning algorithms aim at discovering better representations of the inputs provided during training. Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.\nFeature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labelled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabelled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorisation and various forms of clustering.\nManifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.\nFeature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\n\n\n==== Sparse dictionary learning ====\n\nSparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.\n\n\n==== Anomaly detection ====\n\nIn data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.\nIn particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.\nThree broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabelled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labelled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behaviour from a given normal training data set and then test the likelihood of a test instance to be generated by the model.\n\n\n==== Robot learning ====\nRobot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning, and finally meta-learning (e.g. MAML).", "mimetype": "text/plain", "start_char_idx": 20321, "end_char_idx": 25742, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fd6869ce-427d-409f-ae3d-ba81d9e7bc4b": {"__data__": {"id_": "fd6869ce-427d-409f-ae3d-ba81d9e7bc4b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "233488", "node_type": "4", "metadata": {}, "hash": "12e75e56610f6a3e55b7def2899d987437eeca791453a796249ee4a51d739b23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb30143a-e361-4f5b-9188-a63d2f9bffb2", "node_type": "1", "metadata": {}, "hash": "66e5d151c42e890f2eeb2fd6c47e9f07d2fbaa45326fe68730c1b3a7e2061b54", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "854b52f3-a071-4de2-a390-59ee4dce7694", "node_type": "1", "metadata": {}, "hash": "38493ce3c337dbf140869bf6a40770a001021913960477a18fe3208928e53671", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "==== Robot learning ====\nRobot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning, and finally meta-learning (e.g. MAML).\n\n\n==== Association rules ====\n\nAssociation rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".\nRule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilisation of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.\nBased on the concept of strong rules, Rakesh Agrawal, Tomasz Imieli\u0144ski and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule \n  \n    \n      \n        {\n        \n          o\n          n\n          i\n          o\n          n\n          s\n          ,\n          p\n          o\n          t\n          a\n          t\n          o\n          e\n          s\n        \n        }\n        \u21d2\n        {\n        \n          b\n          u\n          r\n          g\n          e\n          r\n        \n        }\n      \n    \n    {\\displaystyle \\{\\mathrm {onions,potatoes} \\}\\Rightarrow \\{\\mathrm {burger} \\}}\n  \n found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\nLearning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.\nInductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.\nInductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting. Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples. The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.\n\n\n== Models ==\nA machine learning model is a type of mathematical model that, once \"trained\" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimise errors in its predictions. By extension, the term \"model\" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.\nVarious types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection.", "mimetype": "text/plain", "start_char_idx": 25549, "end_char_idx": 30316, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "854b52f3-a071-4de2-a390-59ee4dce7694": {"__data__": {"id_": "854b52f3-a071-4de2-a390-59ee4dce7694", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "233488", "node_type": "4", "metadata": {}, "hash": "12e75e56610f6a3e55b7def2899d987437eeca791453a796249ee4a51d739b23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd6869ce-427d-409f-ae3d-ba81d9e7bc4b", "node_type": "1", "metadata": {}, "hash": "b34717786220a0887c24cfaa2d6710db96795ade1833c7c4fc3d78842231cbbc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "baf65ae8-78ab-4f79-bbee-240ad42fd2fd", "node_type": "1", "metadata": {}, "hash": "5f064d7f0cdbf86d5135a1514be46a74712a14cb06a57a6fb4f0df6dc7eb6980", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "== Models ==\nA machine learning model is a type of mathematical model that, once \"trained\" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimise errors in its predictions. By extension, the term \"model\" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.\nVarious types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection.\n\n\n=== Artificial neural networks ===\n\nArtificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules.\nAn ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.\nThe original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\nDeep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.\n\n\n=== Decision trees ===\n\nDecision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.\n\n\n=== Random forest regression ===\nRandom forest regression (RFR) falls under umbrella of decision tree-based models. RFR is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and to avoid overfitting.  To build decision trees, RFR uses bootstrapped sampling, for instance each decision tree is trained on random data of from training set. This random selection of RFR for training enables model to reduce bias predictions and achieve accuracy. RFR generates independent decision trees, and it can work on single output data as well multiple regressor task. This makes RFR compatible to be used in various application.\n\n\n=== Support-vector machines ===\n\nSupport-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category. An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.", "mimetype": "text/plain", "start_char_idx": 29656, "end_char_idx": 34993, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "baf65ae8-78ab-4f79-bbee-240ad42fd2fd": {"__data__": {"id_": "baf65ae8-78ab-4f79-bbee-240ad42fd2fd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "233488", "node_type": "4", "metadata": {}, "hash": "12e75e56610f6a3e55b7def2899d987437eeca791453a796249ee4a51d739b23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "854b52f3-a071-4de2-a390-59ee4dce7694", "node_type": "1", "metadata": {}, "hash": "38493ce3c337dbf140869bf6a40770a001021913960477a18fe3208928e53671", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9811952-a229-4c7b-b04e-baa78fd8419e", "node_type": "1", "metadata": {}, "hash": "068e7b69ef508fa775bda8e370f6894567fe6bce251a3dbbd65d39211132156f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "=== Support-vector machines ===\n\nSupport-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category. An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n\n\n=== Regression analysis ===\n\nRegression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularisation methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.\nMultivariate linear regression extends the concept of linear regression to handle multiple dependent variables simultaneously. This approach estimates the relationships between a set of input variables and several output variables by fitting a multidimensional linear model. It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images, which are inherently multi-dimensional.\n\n\n=== Bayesian networks ===\n\nA Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalisations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.\n\n\n=== Gaussian processes ===\n\nA Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations.\nGiven a set of observed points, or input\u2013output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point.\nGaussian processes are popular surrogate models in Bayesian optimisation used to do hyperparameter optimisation.\n\n\n=== Genetic algorithms ===\n\nA genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s. Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.\n\n\n=== Belief functions ===\n\nThe theory of belief functions, also referred to as evidence theory or Dempster\u2013Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g.,  Dempster's rule of combination), just like how in a pmf-based Bayesian approach would combine probabilities. However, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance and uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving. However, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches.\n\n\n=== Rule-based models ===\n\nRule-based machine learning (RBML) is a branch of machine learning that automatically discovers and learns 'rules' from data. It provides interpretable models, making it useful for decision-making in fields like healthcare, fraud detection, and cybersecurity. Key RBML techniques includes learning classifier systems, association rule learning, artificial immune systems, and other similar models. These methods extract patterns from data and evolve rules over time.", "mimetype": "text/plain", "start_char_idx": 34223, "end_char_idx": 40046, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b9811952-a229-4c7b-b04e-baa78fd8419e": {"__data__": {"id_": "b9811952-a229-4c7b-b04e-baa78fd8419e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "233488", "node_type": "4", "metadata": {}, "hash": "12e75e56610f6a3e55b7def2899d987437eeca791453a796249ee4a51d739b23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "baf65ae8-78ab-4f79-bbee-240ad42fd2fd", "node_type": "1", "metadata": {}, "hash": "5f064d7f0cdbf86d5135a1514be46a74712a14cb06a57a6fb4f0df6dc7eb6980", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6cd378c8-683e-449f-8b72-e4df40ce35d5", "node_type": "1", "metadata": {}, "hash": "d261f12302f075c79305f7a688d9acb6b9a6ef6550adf6f7d09ff99c5d53529a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "=== Rule-based models ===\n\nRule-based machine learning (RBML) is a branch of machine learning that automatically discovers and learns 'rules' from data. It provides interpretable models, making it useful for decision-making in fields like healthcare, fraud detection, and cybersecurity. Key RBML techniques includes learning classifier systems, association rule learning, artificial immune systems, and other similar models. These methods extract patterns from data and evolve rules over time.\n\n\n=== Training models ===\nTypically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably, becoming integrated within machine learning engineering teams.\n\n\n==== Federated learning ====\n\nFederated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralises the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralised server. This also increases efficiency by decentralising the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.\n\n\n== Applications ==\nThere are many applications for machine learning, including:\n\nIn 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million. Shortly after the prize was awarded, Netflix realised that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly. In 2010, an article in The Wall Street Journal noted the use of machine learning by Rebellion Research to predict the 2008 financial crisis. In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software. In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognised influences among artists. In 2019 Springer Nature published the first research book created using machine learning. In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19. Machine learning was recently applied to predict the pro-environmental behaviour of travellers. Recently, machine learning technology was also applied to optimise smartphone's performance and thermal behaviour based on the user's interaction with the phone. When applied correctly, machine learning algorithms (MLAs) can utilise a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS.\nRecent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.\nMachine Learning is becoming a useful tool to investigate and predict evacuation decision making in large scale and small scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes. Other applications have been focusing on pre evacuation decisions in building fires. \nMachine learning is also emerging as a promising tool in geotechnical engineering, where it is used to support tasks such as ground classification, hazard prediction, and site characterization. Recent research emphasizes a move toward data-centric methods in this field, where machine learning is not a replacement for engineering judgment, but a way to enhance it using site-specific data and patterns.", "mimetype": "text/plain", "start_char_idx": 39553, "end_char_idx": 44659, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6cd378c8-683e-449f-8b72-e4df40ce35d5": {"__data__": {"id_": "6cd378c8-683e-449f-8b72-e4df40ce35d5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "233488", "node_type": "4", "metadata": {}, "hash": "12e75e56610f6a3e55b7def2899d987437eeca791453a796249ee4a51d739b23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9811952-a229-4c7b-b04e-baa78fd8419e", "node_type": "1", "metadata": {}, "hash": "068e7b69ef508fa775bda8e370f6894567fe6bce251a3dbbd65d39211132156f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf1e9d36-e7eb-4d6f-b578-b423df61c8af", "node_type": "1", "metadata": {}, "hash": "bbefd6f6935096165fbbebac2921c94ecaa22e479153937367bc5c2606932f09", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "== Limitations ==\nAlthough machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results. Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.\nThe \"black box theory\" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data. The House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual's life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes.\nIn 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested. Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users.\nMachine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.\n\n\n=== Explainability ===\n\nExplainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI. It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision. By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.\n\n\n=== Overfitting ===\n\nSettling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalising the theory in accordance with how complex the theory is.\n\n\n=== Other limitations and vulnerabilities ===\nLearners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers often do not primarily make judgements from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies.\nAdversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel. Machine learning models are often vulnerable to manipulation or evasion via adversarial machine learning.\nResearchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories \"spam\" and well-visible \"not spam\" of posts) machine learning models that are often developed or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.\n\n\n== Model assessments ==\nClassification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.\nIn addition to overall accuracy, investigators frequently report sensitivity and specificity meaning true positive rate (TPR) and true negative rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. Receiver operating characteristic (ROC) along with the accompanying Area Under the ROC Curve (AUC) offer additional tools for classification model assessment. Higher AUC is associated with a better performing model.\n\n\n== Ethics ==", "mimetype": "text/plain", "start_char_idx": 44662, "end_char_idx": 49963, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cf1e9d36-e7eb-4d6f-b578-b423df61c8af": {"__data__": {"id_": "cf1e9d36-e7eb-4d6f-b578-b423df61c8af", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "233488", "node_type": "4", "metadata": {}, "hash": "12e75e56610f6a3e55b7def2899d987437eeca791453a796249ee4a51d739b23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6cd378c8-683e-449f-8b72-e4df40ce35d5", "node_type": "1", "metadata": {}, "hash": "d261f12302f075c79305f7a688d9acb6b9a6ef6550adf6f7d09ff99c5d53529a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "398628d2-320e-4a1b-9bac-9e94455478e6", "node_type": "1", "metadata": {}, "hash": "bef16661358e0a4c0e51d3f2bc1eac8ab23bdd915c0400d141aec5b712fe087f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "== Ethics ==\n\n\n=== Bias ===\n\nDifferent machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.\nSystems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitising cultural prejudices. For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and that this program had denied nearly 60 candidates who were found to either be women or have non-European sounding names. Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants. Another example includes predictive policing company Geolitica's predictive algorithm that resulted in \"disproportionately high levels of over-policing in low-income and minority communities\" after being trained with historical crime data.\nWhile responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases. In fact, according to research carried out by the Computing Research Association (CRA) in 2021, \"female faculty merely make up 16.1%\" of all faculty members who focus on AI among several universities around the world. Furthermore, among the group of \"new U.S. resident AI PhD graduates,\" 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.\nLanguage models learned from data have been shown to contain human-like biases. Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases. In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.\nIn an experiment carried out by ProPublica, an investigative journalism organisation, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged \"black defendants high risk twice as often as white defendants\". In 2015, Google Photos once tagged a couple of black people as gorillas, which caused controversy. The gorilla label was subsequently removed, and in 2023, it still cannot recognise gorillas. Similar issues with recognising non-white people have been found in many other systems.\nBecause of such challenges, the effective use of machine learning may take longer to be adopted in other domains. Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who said that \"[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and\u2014most importantly\u2014it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\"\n\n\n=== Financial incentives ===\nThere are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.\n\n\n== Hardware ==\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.", "mimetype": "text/plain", "start_char_idx": 49951, "end_char_idx": 54803, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "398628d2-320e-4a1b-9bac-9e94455478e6": {"__data__": {"id_": "398628d2-320e-4a1b-9bac-9e94455478e6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "233488", "node_type": "4", "metadata": {}, "hash": "12e75e56610f6a3e55b7def2899d987437eeca791453a796249ee4a51d739b23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf1e9d36-e7eb-4d6f-b578-b423df61c8af", "node_type": "1", "metadata": {}, "hash": "bbefd6f6935096165fbbebac2921c94ecaa22e479153937367bc5c2606932f09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c27ec39-cd53-4b67-bd76-92d14f4d9412", "node_type": "1", "metadata": {}, "hash": "fde3e47dc78cad06ddfa255e8aedca745233072652929a34d38078c1a43b9c7d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "== Hardware ==\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.\n\n\n=== Tensor Processing Units (TPUs) ===\nTensor Processing Units (TPUs) are specialised hardware accelerators developed by Google specifically for machine learning workloads. Unlike general-purpose GPUs and FPGAs, TPUs are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. They are widely used in Google Cloud AI services and large-scale machine learning models like Google's DeepMind AlphaFold and large language models. TPUs leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency. Since their introduction in 2016, TPUs have become a key component of AI infrastructure, especially in cloud-based environments.\n\n\n=== Neuromorphic computing ===\nNeuromorphic computing refers to a class of computing systems designed to emulate the structure and functionality of biological neural networks. These systems may be implemented through software-based simulations on conventional hardware or through specialised hardware architectures.\n\n\n==== Physical neural networks ====\nA physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of neural synapses. The term \"physical neural network\" highlights the use of physical hardware for computation, as opposed to software-based implementations. It broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses.\n\n\n=== Embedded machine learning ===\nEmbedded machine learning is a sub-field of machine learning where models are deployed on embedded systems with limited computing resources, such as wearable computers, edge devices and microcontrollers. Running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets. Embedded machine learning can be achieved through various techniques, such as hardware acceleration, approximate computing, and model optimisation. Common optimisation techniques include pruning, quantisation, knowledge distillation, low-rank factorisation, network architecture search, and parameter sharing.\n\n\n== Software ==\nSoftware suites containing a variety of machine learning algorithms include the following:\n\n\n=== Free and open-source software ===\n\n\n=== Proprietary software with free and open-source editions ===\nKNIME\nRapidMiner\n\n\n=== Proprietary software ===\n\n\n== Journals ==\nJournal of Machine Learning Research\nMachine Learning\nNature Machine Intelligence\nNeural Computation\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n\n\n== Conferences ==\nAAAI Conference on Artificial Intelligence\nAssociation for Computational Linguistics (ACL)\nEuropean Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)\nInternational Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB)\nInternational Conference on Machine Learning (ICML)\nInternational Conference on Learning Representations (ICLR)\nInternational Conference on Intelligent Robots and Systems (IROS)\nConference on Knowledge Discovery and Data Mining (KDD)\nConference on Neural Information Processing Systems (NeurIPS)\n\n\n== See also ==\nAutomated machine learning \u2013 Process of automating the application of machine learning\nBig data \u2013 Extremely large or complex datasets\nDeep learning \u2014 branch of ML concerned with artificial neural networks\nDifferentiable programming \u2013 Programming paradigm\nList of datasets for machine-learning research\nM-theory (learning framework)\nMachine unlearning\nSolomonoff's theory of inductive inference \u2013 Mathematical theory\n\n\n== References ==", "mimetype": "text/plain", "start_char_idx": 54128, "end_char_idx": 58697, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5c27ec39-cd53-4b67-bd76-92d14f4d9412": {"__data__": {"id_": "5c27ec39-cd53-4b67-bd76-92d14f4d9412", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "233488", "node_type": "4", "metadata": {}, "hash": "12e75e56610f6a3e55b7def2899d987437eeca791453a796249ee4a51d739b23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "398628d2-320e-4a1b-9bac-9e94455478e6", "node_type": "1", "metadata": {}, "hash": "bef16661358e0a4c0e51d3f2bc1eac8ab23bdd915c0400d141aec5b712fe087f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "== Conferences ==\nAAAI Conference on Artificial Intelligence\nAssociation for Computational Linguistics (ACL)\nEuropean Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)\nInternational Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB)\nInternational Conference on Machine Learning (ICML)\nInternational Conference on Learning Representations (ICLR)\nInternational Conference on Intelligent Robots and Systems (IROS)\nConference on Knowledge Discovery and Data Mining (KDD)\nConference on Neural Information Processing Systems (NeurIPS)\n\n\n== See also ==\nAutomated machine learning \u2013 Process of automating the application of machine learning\nBig data \u2013 Extremely large or complex datasets\nDeep learning \u2014 branch of ML concerned with artificial neural networks\nDifferentiable programming \u2013 Programming paradigm\nList of datasets for machine-learning research\nM-theory (learning framework)\nMachine unlearning\nSolomonoff's theory of inductive inference \u2013 Mathematical theory\n\n\n== References ==\n\n\n== Sources ==\nDomingos, Pedro (22 September 2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. ISBN 978-0465065707.\nNilsson, Nils (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann. ISBN 978-1-55860-467-4. Archived from the original on 26 July 2020. Retrieved 18 November 2019.\nPoole, David; Mackworth, Alan; Goebel, Randy (1998). Computational Intelligence: A Logical Approach. New York: Oxford University Press. ISBN 978-0-19-510270-3. Archived from the original on 26 July 2020. Retrieved 22 August 2020.\nRussell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2.\n\n\n== Further reading ==\n\n\n== External links ==\nInternational Machine Learning Society\nmloss is an academic database of open-source machine learning software.", "mimetype": "text/plain", "start_char_idx": 57618, "end_char_idx": 59605, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5b76db97-4452-45bc-8331-0272147563ac": {"__data__": {"id_": "5b76db97-4452-45bc-8331-0272147563ac", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40409788", "node_type": "4", "metadata": {}, "hash": "e3dc56b9ef42af1f1a69930bccf0dad09e4ff8d7b0e8e4a7e4ef9ff14c03a388", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c17c2500-6152-48ba-96e5-5e34a2575c66", "node_type": "1", "metadata": {}, "hash": "ec25a31db27e14def7d2876f2be3b484fc90731068663d84ebc1819cc9e8425f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A convolutional neural network (CNN) is a type of feedforward neural network that learns features via filter (or kernel) optimization. This type of deep learning network has been applied to process and make predictions from many different types of data including text, images and audio. Convolution-based networks are the de-facto standard in deep learning-based approaches to computer vision and image processing, and have only recently been replaced\u2014in some cases\u2014by newer deep learning architectures such as the transformer.\nVanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections. For example, for each neuron in the fully-connected layer, 10,000 weights would be required for processing an image sized 100 \u00d7 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels, only 25 weights for each convolutional layer are required to process 5x5-sized tiles. Higher-layer features are extracted from wider context windows, compared to lower-layer features.\nSome applications of CNNs include: \n\nimage and video recognition,\nrecommender systems,\nimage classification,\nimage segmentation,\nmedical image analysis,\nnatural language processing,\nbrain\u2013computer interfaces, and\nfinancial time series.\nCNNs are also known as shift invariant or space invariant artificial neural networks, based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input.\nFeedforward neural networks are usually fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"full connectivity\" of these networks makes them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) Robust datasets also increase the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set.\nConvolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\nCNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This simplifies and automates the process, enhancing efficiency and scalability overcoming human-intervention bottlenecks.\n\n\n== Architecture ==\n\nA convolutional neural network consists of an input layer, hidden layers and an output layer. In a convolutional neural network, the hidden layers include one or more layers that perform convolutions. Typically this includes a layer that performs a dot product of the convolution kernel with the layer's input matrix. This product is usually the Frobenius inner product, and its activation function is commonly ReLU. As the convolution kernel slides along the input matrix for the layer, the convolution operation generates a feature map, which in turn contributes to the input of the next layer. This is followed by other layers such as pooling layers, fully connected layers, and normalization layers.\nHere it should be noted how close a convolutional neural network is to a matched filter.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3963, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c17c2500-6152-48ba-96e5-5e34a2575c66": {"__data__": {"id_": "c17c2500-6152-48ba-96e5-5e34a2575c66", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40409788", "node_type": "4", "metadata": {}, "hash": "e3dc56b9ef42af1f1a69930bccf0dad09e4ff8d7b0e8e4a7e4ef9ff14c03a388", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b76db97-4452-45bc-8331-0272147563ac", "node_type": "1", "metadata": {}, "hash": "bff1ffc7829a4df7a9f0a99b2f40bab260abe2c38a7698cb8ce9c77184b12417", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6f522fa-dd3d-474a-83f9-cdc03e15cf4d", "node_type": "1", "metadata": {}, "hash": "bda2ebb583c9a8b676b2ed17a0659b8ce0f799f84dc1716f9625a1c6cfa61bf1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "== Architecture ==\n\nA convolutional neural network consists of an input layer, hidden layers and an output layer. In a convolutional neural network, the hidden layers include one or more layers that perform convolutions. Typically this includes a layer that performs a dot product of the convolution kernel with the layer's input matrix. This product is usually the Frobenius inner product, and its activation function is commonly ReLU. As the convolution kernel slides along the input matrix for the layer, the convolution operation generates a feature map, which in turn contributes to the input of the next layer. This is followed by other layers such as pooling layers, fully connected layers, and normalization layers.\nHere it should be noted how close a convolutional neural network is to a matched filter.\n\n\n=== Convolutional layers ===\nIn a CNN, the input is a tensor with shape:\n(number of inputs) \u00d7 (input height) \u00d7 (input width) \u00d7 (input channels)\nAfter passing through a convolutional layer, the image becomes abstracted to a feature map, also called an activation map, with shape:\n(number of inputs) \u00d7 (feature map height) \u00d7 (feature map width) \u00d7 (feature map channels).\nConvolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus. Each convolutional neuron processes data only for its receptive field. \n\nAlthough fully connected feedforward neural networks can be used to learn features and classify data, this architecture is generally impractical for larger inputs (e.g., high-resolution images), which would require massive numbers of neurons because each pixel is a relevant input feature. A fully connected layer for an image of size 100 \u00d7 100 has 10,000 weights for each neuron in the second layer. Convolution reduces the number of free parameters, allowing the network to be deeper. For example, using a 5 \u00d7 5 tiling region, each with the same shared weights, requires only 25 neurons. Using shared weights means there are many fewer parameters, which helps avoid the vanishing gradients and exploding gradients problems seen during backpropagation in earlier neural networks.\nTo speed processing, standard convolutional layers can be replaced by depthwise separable convolutional layers, which are based on a depthwise convolution followed by a pointwise convolution. The depthwise convolution is a spatial convolution applied independently over each channel of the input tensor, while the pointwise convolution is a standard convolution restricted to the use of \n  \n    \n      \n        1\n        \u00d7\n        1\n      \n    \n    {\\displaystyle 1\\times 1}\n  \n kernels.\n\n\n=== Pooling layers ===\nConvolutional networks may include local and/or global pooling layers along with traditional convolutional layers. Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, tiling sizes such as 2 \u00d7 2 are commonly used. Global pooling acts on all the neurons of the feature map. There are two common types of pooling in popular use: max and average. Max pooling uses the maximum value of each local cluster of neurons in the feature map, while average pooling takes the average value.\n\n\n=== Fully connected layers ===\nFully connected layers connect every neuron in one layer to every neuron in another layer. It is the same as a traditional multilayer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images.\n\n\n=== Receptive field ===\nIn neural networks, each neuron receives input from some number of locations in the previous layer. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. Typically the area is a square (e.g. 5 by 5 neurons). Whereas, in a fully connected layer, the receptive field is the entire previous layer. Thus, in each convolutional layer, each neuron takes input from a larger area in the input than previous layers. This is due to applying the convolution over and over, which takes the value of a pixel into account, as well as its surrounding pixels. When using dilated layers, the number of pixels in the receptive field remains constant, but the field is more sparsely populated as its dimensions grow when combining the effect of several layers.\nTo manipulate the receptive field size as desired, there are some alternatives to the standard convolutional layer. For example, atrous or dilated convolution expands the receptive field size without increasing the number of parameters by interleaving visible and blind regions. Moreover, a single dilated convolutional layer can comprise filters with multiple dilation ratios, thus having a variable receptive field size.", "mimetype": "text/plain", "start_char_idx": 3151, "end_char_idx": 8036, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a6f522fa-dd3d-474a-83f9-cdc03e15cf4d": {"__data__": {"id_": "a6f522fa-dd3d-474a-83f9-cdc03e15cf4d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40409788", "node_type": "4", "metadata": {}, "hash": "e3dc56b9ef42af1f1a69930bccf0dad09e4ff8d7b0e8e4a7e4ef9ff14c03a388", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c17c2500-6152-48ba-96e5-5e34a2575c66", "node_type": "1", "metadata": {}, "hash": "ec25a31db27e14def7d2876f2be3b484fc90731068663d84ebc1819cc9e8425f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2047fa83-039b-476e-8088-0481597eb1ba", "node_type": "1", "metadata": {}, "hash": "2f251fa8d7d1e0c52028303717f9715af1e44bb4769fe1d5722d9947c7d65a16", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "=== Weights ===\nEach neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning consists of iteratively adjusting these biases and weights.\nThe vectors of weights and biases are called filters and represent particular features of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the memory footprint because a single bias and a single vector of weights are used across all receptive fields that share that filter, as opposed to each receptive field having its own bias and vector weighting.\n\n\n=== Deconvolutional ===\n\nA deconvolutional neural network is essentially the reverse of a CNN. It consists of deconvolutional layers and unpooling layers. \nA deconvolutional layer is the transpose of a convolutional layer. Specifically, a convolutional layer can be written as a multiplication with a matrix, and a deconvolutional layer is multiplication with the transpose of that matrix.\nAn unpooling layer expands the layer. The max-unpooling layer is the simplest, as it simply copies each entry multiple times. For example, a 2-by-2 max-unpooling layer is \n  \n    \n      \n        [\n        x\n        ]\n        \u21a6\n        \n          \n            [\n            \n              \n                \n                  x\n                \n                \n                  x\n                \n              \n              \n                \n                  x\n                \n                \n                  x\n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle [x]\\mapsto {\\begin{bmatrix}x&x\\\\x&x\\end{bmatrix}}}\n  \n.\nDeconvolution layers are used in image generators. By default, it creates periodic checkerboard artifact, which can be fixed by upscale-then-convolve.\n\n\n== History ==\nCNN are often compared to the way the brain achieves vision processing in living organisms.\n\n\n=== Receptive fields in the visual cortex ===\nWork by Hubel and Wiesel in the 1950s and 1960s showed that cat visual cortices contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field. Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space. The cortex in each hemisphere represents the contralateral visual field.\nTheir 1968 paper identified two basic visual cell types in the brain:\n\nsimple cells, whose output is maximized by straight edges having particular orientations within their receptive field\ncomplex cells, which have larger receptive fields, whose output is insensitive to the exact position of the edges in the field.\nHubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.\n\n\n=== Fukushima's analog threshold elements in a vision model ===\nIn 1969, Kunihiko Fukushima introduced a multilayer visual feature detection network, inspired by the above-mentioned work of Hubel and Wiesel, in which \"All the elements in one layer have the same set of interconnecting coefficients; the arrangement of the elements and their interconnections are all homogeneous over a given layer.\"  This is the essential core of a convolutional network, but the weights were not trained.  In the same paper, Fukushima also introduced the ReLU (rectified linear unit) activation function.", "mimetype": "text/plain", "start_char_idx": 8039, "end_char_idx": 11823, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2047fa83-039b-476e-8088-0481597eb1ba": {"__data__": {"id_": "2047fa83-039b-476e-8088-0481597eb1ba", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40409788", "node_type": "4", "metadata": {}, "hash": "e3dc56b9ef42af1f1a69930bccf0dad09e4ff8d7b0e8e4a7e4ef9ff14c03a388", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6f522fa-dd3d-474a-83f9-cdc03e15cf4d", "node_type": "1", "metadata": {}, "hash": "bda2ebb583c9a8b676b2ed17a0659b8ce0f799f84dc1716f9625a1c6cfa61bf1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e231a92a-1955-453c-8dcc-6e1d242d43f0", "node_type": "1", "metadata": {}, "hash": "007afd1be5de0db82730cee4e08eac3223697ed7d8cc657a2c6d95ec54e373d9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "=== Fukushima's analog threshold elements in a vision model ===\nIn 1969, Kunihiko Fukushima introduced a multilayer visual feature detection network, inspired by the above-mentioned work of Hubel and Wiesel, in which \"All the elements in one layer have the same set of interconnecting coefficients; the arrangement of the elements and their interconnections are all homogeneous over a given layer.\"  This is the essential core of a convolutional network, but the weights were not trained.  In the same paper, Fukushima also introduced the ReLU (rectified linear unit) activation function. \n\n\n=== Neocognitron, origin of the trainable CNN architecture ===\nThe \"neocognitron\" was introduced by Fukushima in 1980.  The neocognitron introduced the two basic types of layers:\n\n\"S-layer\": a shared-weights receptive-field layer, later known as a convolutional layer, which contains units whose receptive fields cover a patch of the previous layer. A shared-weights receptive-field group (a \"plane\" in neocognitron terminology) is often called a filter, and a layer typically has several such filters.\n\"C-layer\": a downsampling layer that contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes a weighted average of the activations of the units in its patch, and applies inhibition (divisive normalization) pooled from a somewhat larger patch and across different filters in a layer, and applies a saturating activation function. The patch weights are nonnegative and are not trainable in the original neocognitron. The downsampling and competitive inhibition help to classify features and objects in visual scenes even when the objects are shifted.\nSeveral supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron. Today, however, the CNN architecture is usually trained through backpropagation.\nFukushima's ReLU activation function was not used in his neocognitron since all the weights were nonnegative; lateral inhibition was used instead. The rectifier has become a very popular activation function for CNNs and deep neural networks in general.\n\n\n=== Convolution in time ===\nThe term \"convolution\" first appears in neural networks in a paper by Toshiteru Homma, Les Atlas, and Robert Marks II at the first Conference on Neural Information Processing Systems in 1987. Their paper replaced multiplication with convolution in time, inherently providing shift invariance, motivated by and connecting more directly to the signal-processing concept of a filter, and demonstrated it on a speech recognition task. They also pointed out that as a data-trainable system, convolution is essentially equivalent to correlation since reversal of the weights does not affect the final learned function (\"For convenience, we denote * as correlation instead of convolution. Note that convolving a(t) with b(t) is equivalent to correlating a(-t) with b(t).\"). Modern CNN implementations typically do correlation and call it convolution, for convenience, as they did here.\n\n\n=== Time delay neural networks ===\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. for phoneme recognition and was an early convolutional network exhibiting shift-invariance. A TDNN is a 1-D convolutional neural net where the convolution is performed along the time axis of the data. It is the first CNN utilizing weight sharing in combination with a training by gradient descent, using backpropagation. Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one.\nTDNNs are convolutional networks that share weights along the temporal dimension. They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant that performs a two-dimensional convolution. Since these TDNNs operated on spectrograms, the resulting phoneme recognition system was invariant to both time and frequency shifts, as with images processed by a neocognitron.\nTDNNs improved the performance of far-distance speech recognition.", "mimetype": "text/plain", "start_char_idx": 11235, "end_char_idx": 15378, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e231a92a-1955-453c-8dcc-6e1d242d43f0": {"__data__": {"id_": "e231a92a-1955-453c-8dcc-6e1d242d43f0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40409788", "node_type": "4", "metadata": {}, "hash": "e3dc56b9ef42af1f1a69930bccf0dad09e4ff8d7b0e8e4a7e4ef9ff14c03a388", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2047fa83-039b-476e-8088-0481597eb1ba", "node_type": "1", "metadata": {}, "hash": "2f251fa8d7d1e0c52028303717f9715af1e44bb4769fe1d5722d9947c7d65a16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c20c5fdf-2b00-4190-8bfa-f9b2502d56ff", "node_type": "1", "metadata": {}, "hash": "d7f3665ce0568050df9042dc14d2b20f266f9ef4d639df8277fe5fe3db1dbed7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "=== Image recognition with CNNs trained by gradient descent ===\nDenker et al. (1989) designed a 2-D CNN system to recognize hand-written ZIP Code numbers. However, the lack of an efficient training method to determine the kernel coefficients of the involved convolutions meant that all the coefficients had to be laboriously hand-designed.\nFollowing the advances in the training of 1-D CNNs by Waibel et al. (1987), Yann LeCun et al. (1989) used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types. \nWei Zhang et al. (1988) used back-propagation to train the convolution kernels of a CNN for alphabets recognition. The model was called shift-invariant pattern recognition neural network before the name CNN was coined later in the early 1990s. Wei Zhang et al. also applied the same CNN without the last fully connected layer for medical image object segmentation (1991) and breast cancer detection in mammograms (1994).\nThis approach became a foundation of modern computer vision.\n\n\n==== Max pooling ====\nIn 1990 Yamaguchi et al. introduced the concept of max pooling, a fixed filtering operation that calculates and propagates the maximum value of a given region. They did so by combining TDNNs with max pooling to realize a speaker-independent isolated word recognition system. In their system they used several TDNNs per word, one for each syllable. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification.\nIn a variant of the neocognitron called the cresceptron, instead of using Fukushima's spatial averaging with inhibition and saturation, J. Weng et al. in 1993 used max pooling, where a downsampling unit computes the maximum of the activations of the units in its patch, introducing this method into the vision field. \nMax pooling is often used in modern CNNs.\n\n\n==== LeNet-5 ====\n\nLeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1995, classifies hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to process higher-resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources.\nIt was superior than other commercial courtesy amount reading systems (as of 1995). The system was integrated in NCR's check reading systems, and fielded in several American banks since June 1996, reading millions of checks per day.\n\n\n=== Shift-invariant neural network ===\nA shift-invariant neural network was proposed by Wei Zhang et al. for image character recognition in 1988. It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer. The model was trained with back-propagation. The training algorithm was further improved in 1991 to improve its generalization ability. The model architecture was modified by removing the last fully connected layer and applied for medical image segmentation (1991) and automatic detection of breast cancer in mammograms (1994).\nA different convolution-based design was proposed in 1988 for application to decomposition of one-dimensional electromyography convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.", "mimetype": "text/plain", "start_char_idx": 15381, "end_char_idx": 18993, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c20c5fdf-2b00-4190-8bfa-f9b2502d56ff": {"__data__": {"id_": "c20c5fdf-2b00-4190-8bfa-f9b2502d56ff", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40409788", "node_type": "4", "metadata": {}, "hash": "e3dc56b9ef42af1f1a69930bccf0dad09e4ff8d7b0e8e4a7e4ef9ff14c03a388", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e231a92a-1955-453c-8dcc-6e1d242d43f0", "node_type": "1", "metadata": {}, "hash": "007afd1be5de0db82730cee4e08eac3223697ed7d8cc657a2c6d95ec54e373d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "704d88ea-295a-4133-93a6-57e65f92611b", "node_type": "1", "metadata": {}, "hash": "d669ab12718c1502c2854ed11cc11c14f95f65bf81edfce7bd40333927f7f4f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "=== Shift-invariant neural network ===\nA shift-invariant neural network was proposed by Wei Zhang et al. for image character recognition in 1988. It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer. The model was trained with back-propagation. The training algorithm was further improved in 1991 to improve its generalization ability. The model architecture was modified by removing the last fully connected layer and applied for medical image segmentation (1991) and automatic detection of breast cancer in mammograms (1994).\nA different convolution-based design was proposed in 1988 for application to decomposition of one-dimensional electromyography convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.\n\n\n=== GPU implementations ===\nAlthough CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations on graphics processing units (GPUs).\nIn 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation on CPU. In 2005, another paper also emphasised the value of GPGPU for machine learning.\nThe first GPU-implementation of a CNN was described in 2006 by K. Chellapilla et al. Their implementation was 4 times faster than an equivalent implementation on CPU. In the same period, GPUs were also used for unsupervised training of deep belief networks.\nIn 2010, Dan Ciresan et al. at IDSIA trained deep feedforward networks on GPUs. In 2011, they extended this to CNNs, accelerating by 60 compared to training CPU. In 2011, the network won an image recognition contest where they achieved superhuman performance for the first time. Then they won more competitions and achieved state of the art on several benchmarks.\nSubsequently, AlexNet, a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012. It was an early catalytic event for the AI boom.\nCompared to the training of CNNs using GPUs, not much attention was given to CPU. (Viebke et al 2019) parallelizes CNN by thread- and SIMD-level parallelism that is available on the Intel Xeon Phi.", "mimetype": "text/plain", "start_char_idx": 18133, "end_char_idx": 20445, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "704d88ea-295a-4133-93a6-57e65f92611b": {"__data__": {"id_": "704d88ea-295a-4133-93a6-57e65f92611b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40409788", "node_type": "4", "metadata": {}, "hash": "e3dc56b9ef42af1f1a69930bccf0dad09e4ff8d7b0e8e4a7e4ef9ff14c03a388", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c20c5fdf-2b00-4190-8bfa-f9b2502d56ff", "node_type": "1", "metadata": {}, "hash": "d7f3665ce0568050df9042dc14d2b20f266f9ef4d639df8277fe5fe3db1dbed7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77a85f82-e1f8-411c-8b92-662ebbb17272", "node_type": "1", "metadata": {}, "hash": "702551c10d7f7170d374907a743e0776b4cac616b46e25a4ecdbd88ed8f4d3b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "== Distinguishing features ==\nIn the past, traditional multilayer perceptron (MLP) models were used for image recognition. However, the full connectivity between nodes caused the curse of dimensionality, and was computationally intractable with higher-resolution images. A 1000\u00d71000-pixel image with RGB color channels has 3 million weights per fully-connected neuron, which is too high to feasibly process efficiently at scale.\n\nFor example, in CIFAR-10, images are only of size 32\u00d732\u00d73 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in the first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200\u00d7200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.\nAlso, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in data with a grid-topology (such as images), both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns.\nConvolutional neural networks are variants of multilayer perceptrons, designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:\n\n3D volumes of neurons. The layers of a CNN have neurons arranged in 3 dimensions: width, height and depth. Where each neuron inside a convolutional layer is connected to only a small region of the layer before it, called a receptive field. Distinct types of layers, both locally and completely connected, are stacked to form a CNN architecture.\nLocal connectivity: following the concept of receptive fields, CNNs exploit spatial locality by enforcing a local connectivity pattern between neurons of adjacent layers. The architecture thus ensures that the learned \"filters\" produce the strongest response to a spatially local input pattern. Stacking many such layers leads to nonlinear filters that become increasingly global (i.e. responsive to a larger region of pixel space) so that the network first creates representations of small parts of the input, then from them assembles representations of larger areas.\nShared weights: In CNNs, each filter is replicated across the entire visual field. These replicated units share the same parameterization (weight vector and bias) and form a feature map. This means that all the neurons in a given convolutional layer respond to the same feature within their specific response field. Replicating units in this way allows for the resulting activation map to be equivariant under shifts of the locations of input features in the visual field, i.e. they grant translational equivariance\u2014given that the layer has a stride of one.\nPooling: In a CNN's pooling layers, feature maps are divided into rectangular sub-regions, and the features in each rectangle are independently down-sampled to a single value, commonly by taking their average or maximum value. In addition to reducing the sizes of feature maps, the pooling operation grants a degree of local translational invariance to the features contained therein, allowing the CNN to be more robust to variations in their positions.\nTogether, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.\n\n\n== Building blocks ==\nA CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below.\n\n\n=== Convolutional layer ===\n\nThe convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the filter entries and the input, producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.\nStacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input. Each entry in an activation map use the same set of parameters that define the filter.\nSelf-supervised learning has been adapted for use in convolutional layers by using sparse patches with a high-mask ratio and a global response normalization layer.", "mimetype": "text/plain", "start_char_idx": 20448, "end_char_idx": 25601, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "77a85f82-e1f8-411c-8b92-662ebbb17272": {"__data__": {"id_": "77a85f82-e1f8-411c-8b92-662ebbb17272", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40409788", "node_type": "4", "metadata": {}, "hash": "e3dc56b9ef42af1f1a69930bccf0dad09e4ff8d7b0e8e4a7e4ef9ff14c03a388", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "704d88ea-295a-4133-93a6-57e65f92611b", "node_type": "1", "metadata": {}, "hash": "d669ab12718c1502c2854ed11cc11c14f95f65bf81edfce7bd40333927f7f4f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a574c213-5872-49ca-91f6-fcc13a2c845f", "node_type": "1", "metadata": {}, "hash": "caa1afbf2cacc6b06ab77bf006b420a4edd36df99d7ef35f3a3512cb05ba4052", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "==== Local connectivity ====\n\nWhen dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a sparse local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume.\nThe extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learned filters produce the strongest response to a spatially local input pattern.\n\n\n==== Spatial arrangement ====\nThree hyperparameters control the size of the output volume of the convolutional layer: the depth, stride, and padding size:\n\nThe depth of the output volume controls the number of neurons in a layer that connect to the same region of the input volume. These neurons learn to activate for different features in the input. For example, if the first convolutional layer takes the raw image as input, then different neurons along the depth dimension may activate in the presence of various oriented edges, or blobs of color.\nStride controls how depth columns around the width and height are allocated. If the stride is 1, then we move the filters one pixel at a time. This leads to heavily overlapping receptive fields between the columns, and to large output volumes. For any integer \n  \n    \n      \n        S\n        >\n        0\n        ,\n      \n    \n    {\\textstyle S>0,}\n  \n a stride S means that the filter is translated S units at a time per output. In practice, \n  \n    \n      \n        S\n        \u2265\n        3\n      \n    \n    {\\textstyle S\\geq 3}\n  \n is rare. A greater stride means smaller overlap of receptive fields and smaller spatial dimensions of the output volume.\nSometimes, it is convenient to pad the input with zeros (or other values, such as the average of the region) on the border of the input volume. The size of this padding is a third hyperparameter. Padding provides control of the output volume's spatial size. In particular, sometimes it is desirable to exactly preserve the spatial size of the input volume, this is commonly referred to as \"same\" padding.\n\nThe spatial size of the output volume is a function of the input volume size \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n, the kernel field size \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n of the convolutional layer neurons, the stride \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n, and the amount of zero padding \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n on the border. The number of neurons that \"fit\" in a given volume is then:\n\n  \n    \n      \n        \n          \n            \n              W\n              \u2212\n              K\n              +\n              2\n              P\n            \n            S\n          \n        \n        +\n        1.\n      \n    \n    {\\displaystyle {\\frac {W-K+2P}{S}}+1.}\n  \n\nIf this number is not an integer, then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in a symmetric way. In general, setting zero padding to be \n  \n    \n      \n        P\n        =\n        (\n        K\n        \u2212\n        1\n        )\n        \n          /\n        \n        2\n      \n    \n    {\\textstyle P=(K-1)/2}\n  \n when the stride is \n  \n    \n      \n        S\n        =\n        1\n      \n    \n    {\\displaystyle S=1}\n  \n ensures that the input volume and output volume will have the same size spatially. However, it is not always completely necessary to use all of the neurons of the previous layer. For example, a neural network designer may decide to use just a portion of padding.", "mimetype": "text/plain", "start_char_idx": 25604, "end_char_idx": 29532, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a574c213-5872-49ca-91f6-fcc13a2c845f": {"__data__": {"id_": "a574c213-5872-49ca-91f6-fcc13a2c845f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40409788", "node_type": "4", "metadata": {}, "hash": "e3dc56b9ef42af1f1a69930bccf0dad09e4ff8d7b0e8e4a7e4ef9ff14c03a388", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77a85f82-e1f8-411c-8b92-662ebbb17272", "node_type": "1", "metadata": {}, "hash": "702551c10d7f7170d374907a743e0776b4cac616b46e25a4ecdbd88ed8f4d3b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02797ee9-f18b-43d9-8c32-466a0b820503", "node_type": "1", "metadata": {}, "hash": "d90c2cbdf4d09b492ee2ea267befbd3b36de85f40b3864c323fbca0c255ecfaf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "==== Parameter sharing ====\nA parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on the assumption that if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. Denoting a single 2-dimensional slice of depth as a depth slice, the neurons in each depth slice are constrained to use the same weights and bias.\nSince all neurons in a single depth slice share the same parameters, the forward pass in each depth slice of the convolutional layer can be computed as a convolution of the neuron's weights with the input volume. Therefore, it is common to refer to the sets of weights as a filter (or a kernel), which is convolved with the input. The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.\nSometimes, the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a \"locally connected layer\".\n\n\n=== Pooling layer ===\n\nAnother important concept of CNNs is pooling, which is used as a form of non-linear down-sampling. Pooling provides downsampling because it reduces the spatial dimensions (height and width) of the input feature maps while retaining the most important information. There are several non-linear functions to implement pooling, where max pooling and average pooling are the most common. Pooling aggregates information from small regions of the input creating partitions of the input feature map, typically using a fixed-size window (like 2x2) and applying a stride (often 2) to move the window across the input. Note that without using a stride greater than 1, pooling would not perform downsampling, as it would simply move the pooling window across the input one step at a time, without reducing the size of the feature map. In other words, the stride is what actually causes the downsampling by determining how much the pooling window moves over the input.\nIntuitively, the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, memory footprint and amount of computation in the network, and hence to also control overfitting. This is known as down-sampling. It is common to periodically insert a pooling layer between successive convolutional layers (each one typically followed by an activation function, such as a ReLU layer) in a CNN architecture.:\u200a460\u2013461\u200a While pooling layers contribute to local translation invariance, they do not provide global translation invariance in a CNN, unless a form of global pooling is used. The pooling layer commonly operates independently on every depth, or slice, of the input and resizes it spatially. A very common form of max pooling is a layer with filters of size 2\u00d72, applied with a stride of 2, which subsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations:\n  \n    \n      \n        \n          f\n          \n            X\n            ,\n            Y\n          \n        \n        (\n        S\n        )\n        =\n        \n          max\n          \n            a\n            ,\n            b\n            =\n            0\n          \n          \n            1\n          \n        \n        \n          S\n          \n            2\n            X\n            +\n            a\n            ,\n            2\n            Y\n            +\n            b\n          \n        \n        .\n      \n    \n    {\\displaystyle f_{X,Y}(S)=\\max _{a,b=0}^{1}S_{2X+a,2Y+b}.}\n  \n\nIn this case, every max operation is over 4 numbers. The depth dimension remains unchanged (this is true for other forms of pooling as well).\nIn addition to max pooling, pooling units can use other functions, such as average pooling or \u21132-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which generally performs better in practice.\nDue to the effects of fast spatial reduction of the size of the representation, there is a recent trend towards using smaller filters or discarding pooling layers altogether.", "mimetype": "text/plain", "start_char_idx": 29535, "end_char_idx": 34444, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "02797ee9-f18b-43d9-8c32-466a0b820503": {"__data__": {"id_": "02797ee9-f18b-43d9-8c32-466a0b820503", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40409788", "node_type": "4", "metadata": {}, "hash": "e3dc56b9ef42af1f1a69930bccf0dad09e4ff8d7b0e8e4a7e4ef9ff14c03a388", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a574c213-5872-49ca-91f6-fcc13a2c845f", "node_type": "1", "metadata": {}, "hash": "caa1afbf2cacc6b06ab77bf006b420a4edd36df99d7ef35f3a3512cb05ba4052", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6cc507a-8204-49e1-92dc-e332369ae740", "node_type": "1", "metadata": {}, "hash": "252de977c7e4754b3005c9a4608b805345e90dc118908f8e879ee2798f2693e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "==== Channel max pooling ====\nA channel max pooling (CMP) operation layer conducts the MP operation along the channel side among the corresponding positions of the consecutive feature maps for the purpose of redundant information elimination. The CMP makes the significant features gather together within fewer channels, which is important for fine-grained image classification that needs more discriminating features. Meanwhile, another advantage of the CMP operation is to make the channel number of feature maps smaller before it connects to the first fully connected (FC) layer. Similar to the MP operation, we denote the input feature maps and output feature maps of a CMP layer as F \u2208 R(C\u00d7M\u00d7N) and C \u2208 R(c\u00d7M\u00d7N), respectively, where C and c are the channel numbers of the input and output feature maps, M and N are the widths and the height of the feature maps, respectively. Note that the CMP operation only changes the channel number of the feature maps. The width and the height of the feature maps are not changed, which is different from the MP operation.\nSee  for reviews for pooling methods.\n\n\n=== ReLU layer ===\nReLU is the abbreviation of rectified linear unit. It was proposed by Alston Householder in 1941, and used in CNN by Kunihiko Fukushima in 1969. ReLU applies the non-saturating activation function \n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        max\n        (\n        0\n        ,\n        x\n        )\n      \n    \n    {\\textstyle f(x)=\\max(0,x)}\n  \n. It effectively removes negative values from an activation map by setting them to zero. It introduces nonlinearity to the decision function and in the overall network without affecting the receptive fields of the convolution layers.\nIn 2011, Xavier Glorot, Antoine Bordes and Yoshua Bengio found that ReLU enables better training of deeper networks, compared to widely used activation functions prior to 2011.\nOther functions can also be used to increase nonlinearity, for example the saturating hyperbolic tangent \n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        tanh\n        \u2061\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)=\\tanh(x)}\n  \n, \n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        \n          |\n        \n        tanh\n        \u2061\n        (\n        x\n        )\n        \n          |\n        \n      \n    \n    {\\displaystyle f(x)=|\\tanh(x)|}\n  \n, and the sigmoid function \n  \n    \n      \n        \u03c3\n        (\n        x\n        )\n        =\n        (\n        1\n        +\n        \n          e\n          \n            \u2212\n            x\n          \n        \n        \n          )\n          \n            \u2212\n            1\n          \n        \n      \n    \n    {\\textstyle \\sigma (x)=(1+e^{-x})^{-1}}\n  \n. ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy.\n\n\n=== Fully connected layer ===\nAfter several convolutional and max pooling layers, the final classification is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation, with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term).\n\n\n=== Loss layer ===\n\nThe \"loss layer\", or \"loss function\", exemplifies how training penalizes the deviation between the predicted output of the network, and the true data labels (during supervised learning). Various loss functions can be used, depending on the specific task.\nThe Softmax loss function is used for predicting a single class of K mutually exclusive classes. Sigmoid cross-entropy loss is used for predicting K independent probability values in \n  \n    \n      \n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle [0,1]}\n  \n. Euclidean loss is used for regressing to real-valued labels \n  \n    \n      \n        (\n        \u2212\n        \u221e\n        ,\n        \u221e\n        )\n      \n    \n    {\\displaystyle (-\\infty ,\\infty )}\n  \n.\n\n\n== Hyperparameters ==\n\nHyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than a standard multilayer perceptron (MLP).\n\n\n=== Padding ===\nPadding is the addition of (typically) 0-valued pixels on the borders of an image. This is done so that the border pixels are not undervalued (lost) from the output because they would ordinarily participate in only a single receptive field instance. The padding applied is typically one less than the corresponding kernel dimension. For example, a convolutional layer using 3x3 kernels would receive a 2-pixel pad, that is 1 pixel on each side of the image.", "mimetype": "text/plain", "start_char_idx": 34447, "end_char_idx": 39261, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d6cc507a-8204-49e1-92dc-e332369ae740": {"__data__": {"id_": "d6cc507a-8204-49e1-92dc-e332369ae740", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40409788", "node_type": "4", "metadata": {}, "hash": "e3dc56b9ef42af1f1a69930bccf0dad09e4ff8d7b0e8e4a7e4ef9ff14c03a388", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02797ee9-f18b-43d9-8c32-466a0b820503", "node_type": "1", "metadata": {}, "hash": "d90c2cbdf4d09b492ee2ea267befbd3b36de85f40b3864c323fbca0c255ecfaf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c185d241-e0d3-425c-b0f5-576dcf2241fd", "node_type": "1", "metadata": {}, "hash": "aaad958f2a02f0aa6beb7bced4a98b5ef621274e5424304d2c7647d42313945c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "== Hyperparameters ==\n\nHyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than a standard multilayer perceptron (MLP).\n\n\n=== Padding ===\nPadding is the addition of (typically) 0-valued pixels on the borders of an image. This is done so that the border pixels are not undervalued (lost) from the output because they would ordinarily participate in only a single receptive field instance. The padding applied is typically one less than the corresponding kernel dimension. For example, a convolutional layer using 3x3 kernels would receive a 2-pixel pad, that is 1 pixel on each side of the image.\n\n\n=== Stride ===\nThe stride is the number of pixels that the analysis window moves on each iteration. A stride of 2 means that each kernel is offset by 2 pixels from its predecessor.\n\n\n=== Number of filters ===\nSince feature map size decreases with depth, layers near the input layer tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the product of feature values va with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.\nThe number of feature maps directly controls the capacity and depends on the number of available examples and task complexity.\n\n\n=== Filter (or Kernel) size ===\nCommon filter sizes found in the literature vary greatly, and are usually chosen based on the data set. Typical filter sizes range from 1x1 to 7x7. As two famous examples, AlexNet used 3x3, 5x5, and 11x11. Inceptionv3 used 1x1, 3x3, and 5x5.\nThe challenge is to find the right level of granularity so as to create abstractions at the proper scale, given a particular data set, and without overfitting.\n\n\n=== Pooling type and size ===\nMax pooling is typically used, often with a 2x2 dimension. This implies that the input is drastically downsampled, reducing processing cost.\nGreater pooling reduces the dimension of the signal, and may result in unacceptable information loss. Often, non-overlapping pooling windows perform best.\n\n\n=== Dilation ===\nDilation involves ignoring pixels within a kernel. This reduces processing memory potentially without significant signal loss. A dilation of 2 on a 3x3 kernel expands the kernel to 5x5, while still processing 9 (evenly spaced) pixels. Specifically, the processed pixels after the dilation are the cells (1,1), (1,3), (1,5), (3,1), (3,3), (3,5), (5,1), (5,3), (5,5), where (i,j) denotes the cell of the i-th row and j-th column in the expanded 5x5 kernel. Accordingly, dilation of 4 expands the kernel to 7x7.\n\n\n== Translation equivariance and aliasing ==\nIt is commonly assumed that CNNs are invariant to shifts of the input. Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed equivariant to translations of the input. However, layers with a stride greater than one ignore the Nyquist\u2013Shannon sampling theorem and might lead to aliasing of the input signal While, in principle, CNNs are capable of implementing anti-aliasing filters, it has been observed that this does not happen in practice, and therefore yield models that are not equivariant to translations.\nFurthermore, if a CNN makes use of fully connected layers, translation equivariance does not imply translation invariance, as the fully connected layers are not invariant to shifts of the input. One solution for complete translation invariance is avoiding any down-sampling throughout the network and applying global average pooling at the last layer. Additionally, several other partial solutions have been proposed, such as anti-aliasing before downsampling operations, spatial transformer networks, data augmentation, subsampling combined with pooling, and capsule neural networks.\n\n\n== Evaluation ==\nThe accuracy of the final model is typically estimated on a sub-part of the dataset set apart at the start, often called a test set. Alternatively, methods such as k-fold cross-validation are applied. Other strategies include using conformal prediction.\n\n\n== Regularization methods ==\n\nRegularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization.\n\n\n=== Empirical ===", "mimetype": "text/plain", "start_char_idx": 38604, "end_char_idx": 43031, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c185d241-e0d3-425c-b0f5-576dcf2241fd": {"__data__": {"id_": "c185d241-e0d3-425c-b0f5-576dcf2241fd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40409788", "node_type": "4", "metadata": {}, "hash": "e3dc56b9ef42af1f1a69930bccf0dad09e4ff8d7b0e8e4a7e4ef9ff14c03a388", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6cc507a-8204-49e1-92dc-e332369ae740", "node_type": "1", "metadata": {}, "hash": "252de977c7e4754b3005c9a4608b805345e90dc118908f8e879ee2798f2693e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df77e359-bc55-4b8a-a505-60bd016b07c7", "node_type": "1", "metadata": {}, "hash": "f914196a2f4d0f4742ac2d2ddd83556efd8581746726cf79c286bd968cd8fb19", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "== Evaluation ==\nThe accuracy of the final model is typically estimated on a sub-part of the dataset set apart at the start, often called a test set. Alternatively, methods such as k-fold cross-validation are applied. Other strategies include using conformal prediction.\n\n\n== Regularization methods ==\n\nRegularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization.\n\n\n=== Empirical ===\n\n\n==== Dropout ====\nBecause networks have so many parameters, they are prone to overfitting. One method to reduce overfitting is dropout, introduced in 2014. At each training stage, individual nodes are either \"dropped out\" of the net (ignored) with probability \n  \n    \n      \n        1\n        \u2212\n        p\n      \n    \n    {\\displaystyle 1-p}\n  \n or kept with probability \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights.\nIn the training stages, \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n is usually 0.5; for input nodes, it is typically much higher because information is directly lost when input nodes are ignored.\nAt testing time after training has finished, we would ideally like to find a sample average of all possible \n  \n    \n      \n        \n          2\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{n}}\n  \n dropped-out networks; unfortunately this is unfeasible for large values of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n. However, we can find an approximation by using the full network with each node's output weighted by a factor of \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n, so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates \n  \n    \n      \n        \n          2\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{n}}\n  \n neural nets, and as such allows for model combination, at test time only a single network needs to be tested.\nBy avoiding training all nodes on all training data, dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical, even for deep neural networks. The technique seems to reduce node interactions, leading them to learn more robust features that better generalize to new data.\n\n\n==== DropConnect ====\nDropConnect is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability \n  \n    \n      \n        1\n        \u2212\n        p\n      \n    \n    {\\displaystyle 1-p}\n  \n. Each unit thus receives input from a random subset of units in the previous layer.\nDropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage.\n\n\n==== Stochastic pooling ====\nA major drawback to dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected.\nEven before dropout, in 2013 a technique called stochastic pooling, the conventional deterministic pooling operations were replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a multinomial distribution, given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches, such as dropout and data augmentation.\nAn alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images, which delivers excellent performance on the MNIST data set. Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below.\n\n\n==== Artificial data ====\n\nBecause the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Because there is often not enough available data to train, especially considering that some part should be spared for later testing, two approaches are to either generate new data from scratch (if possible) or perturb existing data to create new ones. The latter one is used since mid-1990s. For example, input images can be cropped, rotated, or rescaled to create new examples with the same labels as the original training set.\n\n\n=== Explicit ===", "mimetype": "text/plain", "start_char_idx": 42544, "end_char_idx": 47674, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "df77e359-bc55-4b8a-a505-60bd016b07c7": {"__data__": {"id_": "df77e359-bc55-4b8a-a505-60bd016b07c7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40409788", "node_type": "4", "metadata": {}, "hash": "e3dc56b9ef42af1f1a69930bccf0dad09e4ff8d7b0e8e4a7e4ef9ff14c03a388", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c185d241-e0d3-425c-b0f5-576dcf2241fd", "node_type": "1", "metadata": {}, "hash": "aaad958f2a02f0aa6beb7bced4a98b5ef621274e5424304d2c7647d42313945c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9cdc96cf-4934-4d7b-9a92-39bcdb005977", "node_type": "1", "metadata": {}, "hash": "703a30f66fb0b7dd4496e96fa97e0b800bd3ea1454d47c3e5508816479c50854", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "==== Artificial data ====\n\nBecause the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Because there is often not enough available data to train, especially considering that some part should be spared for later testing, two approaches are to either generate new data from scratch (if possible) or perturb existing data to create new ones. The latter one is used since mid-1990s. For example, input images can be cropped, rotated, or rescaled to create new examples with the same labels as the original training set.\n\n\n=== Explicit ===\n\n\n==== Early stopping ====\n\nOne of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted.\n\n\n==== Number of parameters ====\nAnother simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a \"zero norm\".\n\n\n==== Weight decay ====\nA simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant('alpha' hyperparameter), thus increasing the penalty for large weight vectors.\nL2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot.\nL1 regularization is also common. It makes the weight vectors sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs. L1 with L2 regularization can be combined; this is called elastic net regularization.\n\n\n==== Max norm constraints ====\nAnother form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector \n  \n    \n      \n        \n          \n            \n              w\n              \u2192\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {w}}}\n  \n of every neuron to satisfy \n  \n    \n      \n        \u2016\n        \n          \n            \n              w\n              \u2192\n            \n          \n        \n        \n          \u2016\n          \n            2\n          \n        \n        <\n        c\n      \n    \n    {\\displaystyle \\|{\\vec {w}}\\|_{2}<c}\n  \n. Typical values of \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n are order of 3\u20134. Some papers report improvements when using this form of regularization.", "mimetype": "text/plain", "start_char_idx": 47002, "end_char_idx": 50674, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9cdc96cf-4934-4d7b-9a92-39bcdb005977": {"__data__": {"id_": "9cdc96cf-4934-4d7b-9a92-39bcdb005977", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40409788", "node_type": "4", "metadata": {}, "hash": "e3dc56b9ef42af1f1a69930bccf0dad09e4ff8d7b0e8e4a7e4ef9ff14c03a388", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df77e359-bc55-4b8a-a505-60bd016b07c7", "node_type": "1", "metadata": {}, "hash": "f914196a2f4d0f4742ac2d2ddd83556efd8581746726cf79c286bd968cd8fb19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec1046e5-830b-4f74-9d67-13d01d1ecfdf", "node_type": "1", "metadata": {}, "hash": "b163152590e9ffbcb338e2d964e03e99ff4c9fddb445dfaf4b7c100ea6f0af1f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "==== Max norm constraints ====\nAnother form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector \n  \n    \n      \n        \n          \n            \n              w\n              \u2192\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {w}}}\n  \n of every neuron to satisfy \n  \n    \n      \n        \u2016\n        \n          \n            \n              w\n              \u2192\n            \n          \n        \n        \n          \u2016\n          \n            2\n          \n        \n        <\n        c\n      \n    \n    {\\displaystyle \\|{\\vec {w}}\\|_{2}<c}\n  \n. Typical values of \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n are order of 3\u20134. Some papers report improvements when using this form of regularization.\n\n\n== Hierarchical coordinate frames ==\nPooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.\nAn earlier common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.\nThus, one way to represent something is to embed the coordinate frame within it. This allows large features to be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). This approach ensures that the higher-level entity (e.g. face) is present when the lower-level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose (\"pose vectors\") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human visual system imposes coordinate frames in order to represent shapes.\n\n\n== Applications ==", "mimetype": "text/plain", "start_char_idx": 49690, "end_char_idx": 52633, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ec1046e5-830b-4f74-9d67-13d01d1ecfdf": {"__data__": {"id_": "ec1046e5-830b-4f74-9d67-13d01d1ecfdf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40409788", "node_type": "4", "metadata": {}, "hash": "e3dc56b9ef42af1f1a69930bccf0dad09e4ff8d7b0e8e4a7e4ef9ff14c03a388", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9cdc96cf-4934-4d7b-9a92-39bcdb005977", "node_type": "1", "metadata": {}, "hash": "703a30f66fb0b7dd4496e96fa97e0b800bd3ea1454d47c3e5508816479c50854", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b32db49f-4202-46c2-97f5-537a61a58ec2", "node_type": "1", "metadata": {}, "hash": "0cfa6b15c6a0b00307622d53db5d7d8dc3caca30abfd65393dbc14d76215e434", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "== Applications ==\n\n\n=== Image recognition ===\nCNNs are often used in image recognition systems. In 2012, an error rate of 0.23% on the MNIST database was reported. Another paper on using CNN for image classification reported that the learning process was \"surprisingly fast\"; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database. Subsequently, a similar CNN called AlexNet won the ImageNet Large Scale Visual Recognition Challenge 2012.\nWhen applied to facial recognition, CNNs achieved a large decrease in error rate. Another paper reported a 97.6% recognition rate on \"5,600 still images of more than 10 subjects\". CNNs were used to assess video quality in an objective way after manual training; the resulting system had a very low root mean square error.\nThe ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014, a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.\nIn 2015, a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded, with competitive performance. The network was trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.\n\n\n=== Video analysis ===\nCompared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space. Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream. Long short-term memory (LSTM) recurrent units are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies. Unsupervised learning schemes for training spatio-temporal features have been introduced, based on Convolutional Gated Restricted Boltzmann Machines and Independent Subspace Analysis. Its application can be seen in text-to-video model.\n\n\n=== Natural language processing ===\nCNNs have also been explored for natural language processing. CNN models are effective for various NLP problems and achieved excellent results in semantic parsing, search query retrieval, sentence modeling, classification, prediction and other traditional NLP tasks.\nCompared to traditional language processing methods such as recurrent neural networks, CNNs can represent different contextual realities of language that do not rely on a series-sequence assumption, while RNNs are better suitable when classical time series modeling is required.\n\n\n=== Anomaly detection ===\nA CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain.\n\n\n=== Drug discovery ===\nCNNs have been used in drug discovery. Predicting the interaction between molecules and biological proteins can identify potential treatments. In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based drug design. The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures, AtomNet discovers chemical features, such as aromaticity, sp3 carbons, and hydrogen bonding. Subsequently, AtomNet was used to predict novel candidate biomolecules for multiple disease targets, most notably treatments for the Ebola virus and multiple sclerosis.", "mimetype": "text/plain", "start_char_idx": 52615, "end_char_idx": 57501, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b32db49f-4202-46c2-97f5-537a61a58ec2": {"__data__": {"id_": "b32db49f-4202-46c2-97f5-537a61a58ec2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40409788", "node_type": "4", "metadata": {}, "hash": "e3dc56b9ef42af1f1a69930bccf0dad09e4ff8d7b0e8e4a7e4ef9ff14c03a388", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec1046e5-830b-4f74-9d67-13d01d1ecfdf", "node_type": "1", "metadata": {}, "hash": "b163152590e9ffbcb338e2d964e03e99ff4c9fddb445dfaf4b7c100ea6f0af1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "286b5859-fd04-48da-af5b-538650bfcbd0", "node_type": "1", "metadata": {}, "hash": "52c992e9cb923aaf5cee0aca8819eed9c8a036ffacd4aecc5ead7dd923f53bf3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "=== Anomaly detection ===\nA CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain.\n\n\n=== Drug discovery ===\nCNNs have been used in drug discovery. Predicting the interaction between molecules and biological proteins can identify potential treatments. In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based drug design. The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures, AtomNet discovers chemical features, such as aromaticity, sp3 carbons, and hydrogen bonding. Subsequently, AtomNet was used to predict novel candidate biomolecules for multiple disease targets, most notably treatments for the Ebola virus and multiple sclerosis.\n\n\n=== Checkers game ===\nCNNs have been used in the game of checkers. From 1999 to 2001, Fogel and Chellapilla published papers showing how a convolutional neural network could learn to play checkers using co-evolution. The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and the difference in number of pieces between the two sides. Ultimately, the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%. It also earned a win against the program Chinook at its \"expert\" level of play.\n\n\n=== Go ===\nCNNs have been used in computer Go. In December 2014, Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play. Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GNU Go in 97% of games, and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts (about a million positions) per move.\nA couple of CNNs for choosing moves to try (\"policy network\") and evaluating positions (\"value network\") driving MCTS were used by AlphaGo, the first to beat the best human player at the time.\n\n\n=== Time series forecasting ===\nRecurrent neural networks are generally considered the best neural network architectures for time series forecasting (and sequence modeling in general), but recent studies show that convolutional networks can perform comparably or even better. Dilated convolutions might enable one-dimensional convolutional neural networks to effectively learn time series dependences. Convolutions can be implemented more efficiently than RNN-based solutions, and they do not suffer from vanishing (or exploding) gradients. Convolutional networks can provide an improved forecasting performance when there are multiple similar time series to learn from. CNNs can also be applied to further tasks in time series analysis (e.g., time series classification or quantile forecasting).\n\n\n=== Cultural heritage and 3D-datasets ===\nAs archaeological findings such as clay tablets with cuneiform writing are increasingly acquired using 3D scanners, benchmark datasets are becoming available, including HeiCuBeDa providing almost 2000 normalized 2-D and 3-D datasets prepared with the GigaMesh Software Framework. So curvature-based measures are used in conjunction with geometric neural networks (GNNs), e.g. for period classification of those clay tablets being among the oldest documents of human history.\n\n\n== Fine-tuning ==\nFor many applications, training data is not very available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights, this is known as transfer learning. Furthermore, this technique allows convolutional network architectures to successfully be applied to problems with tiny training sets.\n\n\n== Human interpretable explanations ==\nEnd-to-end training and prediction are common practice in computer vision. However, human interpretable explanations are required for critical systems such as a self-driving cars. With recent advances in visual salience, spatial attention, and temporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.\n\n\n== Related architectures ==", "mimetype": "text/plain", "start_char_idx": 56559, "end_char_idx": 61515, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "286b5859-fd04-48da-af5b-538650bfcbd0": {"__data__": {"id_": "286b5859-fd04-48da-af5b-538650bfcbd0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40409788", "node_type": "4", "metadata": {}, "hash": "e3dc56b9ef42af1f1a69930bccf0dad09e4ff8d7b0e8e4a7e4ef9ff14c03a388", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b32db49f-4202-46c2-97f5-537a61a58ec2", "node_type": "1", "metadata": {}, "hash": "0cfa6b15c6a0b00307622d53db5d7d8dc3caca30abfd65393dbc14d76215e434", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "== Fine-tuning ==\nFor many applications, training data is not very available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights, this is known as transfer learning. Furthermore, this technique allows convolutional network architectures to successfully be applied to problems with tiny training sets.\n\n\n== Human interpretable explanations ==\nEnd-to-end training and prediction are common practice in computer vision. However, human interpretable explanations are required for critical systems such as a self-driving cars. With recent advances in visual salience, spatial attention, and temporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.\n\n\n== Related architectures ==\n\n\n=== Deep Q-networks ===\nA deep Q-network (DQN) is a type of deep learning model that combines a deep neural network with Q-learning, a form of reinforcement learning. Unlike earlier reinforcement learning agents, DQNs that utilize CNNs can learn directly from high-dimensional sensory inputs via reinforcement learning.\nPreliminary results were presented in 2014, with an accompanying paper in February 2015. The research described an application to Atari 2600 gaming. Other deep reinforcement learning models preceded it.\n\n\n=== Deep belief networks ===\n\nConvolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training like deep belief networks. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR have been obtained using CDBNs.\n\n\n=== Neural abstraction pyramid ===\nThe feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated, e.g., for semantic segmentation, image reconstruction, and object localization tasks.\n\n\n== Notable libraries ==\nCaffe: A library for convolutional neural networks. Created by the Berkeley Vision and Learning Center (BVLC). It supports both CPU and GPU. Developed in C++, and has Python and MATLAB wrappers.\nDeeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark. A general-purpose deep learning library for the JVM production stack running on a C++ scientific computing engine. Allows the creation of custom layers. Integrates with Hadoop and Kafka.\nDlib: A toolkit for making real world machine learning and data analysis applications in C++.\nMicrosoft Cognitive Toolkit: A deep learning toolkit written by Microsoft with several unique features enhancing scalability over multiple nodes. It supports full-fledged interfaces for training in C++ and Python and with additional support for model inference in C# and Java.\nTensorFlow: Apache 2.0-licensed Theano-like library with support for CPU, GPU, Google's proprietary tensor processing unit (TPU), and mobile devices.\nTheano: The reference deep-learning library for Python with an API largely compatible with the popular NumPy library. Allows user to write symbolic mathematical expressions, then automatically generates their derivatives, saving the user from having to code gradients or backpropagation. These symbolic expressions are automatically compiled to CUDA code for a fast, on-the-GPU implementation.\nTorch: A scientific computing framework with wide support for machine learning algorithms, written in C and Lua.\n\n\n== See also ==\nAttention (machine learning)\nConvolution\nDeep learning\nNatural-language processing\nNeocognitron\nScale-invariant feature transform\nTime delay neural network\nVision processing unit\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\nCS231n: Convolutional Neural Networks for Visual Recognition \u2014 Andrej Karpathy's Stanford computer science course on CNNs in computer vision\nvdumoulin/conv_arithmetic: A technical report on convolution arithmetic in the context of deep learning. Animations of convolutions.", "mimetype": "text/plain", "start_char_idx": 60489, "end_char_idx": 65082, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"233488": {"node_ids": ["e19629e2-aa3b-4eac-be61-af4e39110824", "868a8cb0-3c5e-4def-b00b-56e90ca0de44", "d494ccf7-7264-4781-8bb0-2d4420ec73d8", "1912ec31-be5e-454f-96f1-3122610cfb18", "eb30143a-e361-4f5b-9188-a63d2f9bffb2", "fd6869ce-427d-409f-ae3d-ba81d9e7bc4b", "854b52f3-a071-4de2-a390-59ee4dce7694", "baf65ae8-78ab-4f79-bbee-240ad42fd2fd", "b9811952-a229-4c7b-b04e-baa78fd8419e", "6cd378c8-683e-449f-8b72-e4df40ce35d5", "cf1e9d36-e7eb-4d6f-b578-b423df61c8af", "398628d2-320e-4a1b-9bac-9e94455478e6", "5c27ec39-cd53-4b67-bd76-92d14f4d9412"], "metadata": {}}, "40409788": {"node_ids": ["5b76db97-4452-45bc-8331-0272147563ac", "c17c2500-6152-48ba-96e5-5e34a2575c66", "a6f522fa-dd3d-474a-83f9-cdc03e15cf4d", "2047fa83-039b-476e-8088-0481597eb1ba", "e231a92a-1955-453c-8dcc-6e1d242d43f0", "c20c5fdf-2b00-4190-8bfa-f9b2502d56ff", "704d88ea-295a-4133-93a6-57e65f92611b", "77a85f82-e1f8-411c-8b92-662ebbb17272", "a574c213-5872-49ca-91f6-fcc13a2c845f", "02797ee9-f18b-43d9-8c32-466a0b820503", "d6cc507a-8204-49e1-92dc-e332369ae740", "c185d241-e0d3-425c-b0f5-576dcf2241fd", "df77e359-bc55-4b8a-a505-60bd016b07c7", "9cdc96cf-4934-4d7b-9a92-39bcdb005977", "ec1046e5-830b-4f74-9d67-13d01d1ecfdf", "b32db49f-4202-46c2-97f5-537a61a58ec2", "286b5859-fd04-48da-af5b-538650bfcbd0"], "metadata": {}}}}